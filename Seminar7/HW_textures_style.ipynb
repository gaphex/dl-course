{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture synthesis and artistic style transfer\n",
    "\n",
    "In this homework you are to imlement [A Neural algorithm of artistic style](http://arxiv.org/pdf/1508.06576v2.pdf). This is an extension of [Texture Synthesis Using Convolutional Neural Networks](http://arxiv.org/pdf/1505.07376v3.pdf) method.\n",
    "\n",
    "The core of the method -- VGG and constrained optimization. The constrains are of two types: *content* and *style*. Given a content image **C** and style image **S** we want to generate an image **X** with content from **C** and style (whatever it really means) from **S**. \n",
    "\n",
    "We want to design a loss function for the optimization process. Considering \\[1\\], \\[2\\], an input image is easily invertable from the outputs at intermediate layers. This explains the idea of making an intermediate representation $F_X$ of **X** close to **C** representation $F_C$. \n",
    "\n",
    "$$\n",
    "   L_{content} = || F_X - F_C || \\rightarrow \\min_X\n",
    "$$\n",
    "\n",
    "Note, that representation $F$ preserve spatial information. Idea: let us dismiss it, so we will know what objects are there on the picture, but will not be able to reestablish their localtion. The style can be thought as something independent of content, something we are left with if we let the content off. L. Gatys suggests to dismiss spatial information by computing correlations between the feature maps $F$. If $F$ has dimensions `CxWxH`, then correlation matrix will be `CxC`, and look there's no spatial dimentions. So the style term will be responsible for mathing these correlation (Gram) matrices. \n",
    "\n",
    "$$\n",
    "   L_{style} = || Gram(F_X) - Gram(F_C) || \\rightarrow \\min_X\n",
    "$$\n",
    "\n",
    "And finaly we combine the two.\n",
    "\n",
    "$$\n",
    "   L = \\alpha L_{content} + \\beta L_{style} \\min_X\n",
    "$$\n",
    "\n",
    "Read the paper and the code for the details on layers, features $F$ are got from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little bit of history behind this texture generation method\n",
    "\n",
    "Actually the idea comes from 90th, when mathematical models of texures were developed \\[3\\]. They defined a probabolistic model for texture generation. They used an idea, that two images are indeed two samples of a particular texture iff their statistics match. The statistics used are histograms of given texture $I$ filtered with a number of filters: $\\{hist(F_i * I), \\quad i = 1,\\dots, k\\}$. And whatever image has the same statistics is thought as a sample of texture $I$. The main drawback was the Gibbs sampling was employed (which is very slow). \\[4\\] suggested exactly the scheme we use now: starting from a random image, let's adjust its statistics iteratively so they match the desired. \n",
    "\n",
    "Now, what is changed: the filters. \\[4\\] used carefully crafted set of filters, and now we use neural network based non-linear filters. We still use the idea of matching statistics, but the statistics improved. \n",
    "\n",
    "\\[1\\] *A.Mahendran, A.Vedaldi [Understanding Deep Image Representations by Inverting Them](https://www.robots.ox.ac.uk/~vgg/publications/2015/Mahendran15/mahendran15.pdf)*\n",
    "\n",
    "\\[2\\] *A.Dosovitsky, T.Brox [Inverting Visual Representations with Convolutional Networks](http://arxiv.org/pdf/1506.02753v3.pdf)*\n",
    "\n",
    "\\[3\\] *Zhu et. al. Filters, 1997 [Random Fields and Maximum Entropy (FRAME):\n",
    "Towards a Unified Theory for Texture Modeling](http://www.stat.ucla.edu/~ywu/research/papers/ijcv.pdf)*\n",
    "\n",
    "\\[4\\] *Portilla & Simoncelli, 2000  [A Parametric Texture Model Based on Joint Statistics\n",
    "of Complex Wavelet Coefficients](http://www.cns.nyu.edu/pub/lcv/portilla99-reprint.pdf)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prevent you from technical problems, you may use a [complete code for the method](https://github.com/Lasagne/Recipes/tree/master/examples/styletransfer). \n",
    "Your task will be to play around with it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First part\n",
    "**Common mandatory part**:\n",
    "- Generate your favourite texture (please, do not use starry night). All you need to do is to set content weight to 0.  \n",
    "- Stylize your favourite photo with your favourite style (hope you use something interesting).\n",
    "- Give an explanation for matching Gram matrices. What does it mean to minimize distance between them in terms of random variables? Assume a true distripution $P$, and model distibution $Q$. What class does $Q$ belong when matching gram matrices? Show, that $KL (P || Q)$ is minimized when Gram matrices are matched. In other words you need to come up with $Q$ such that $KL$ divergence is minimized when models gram matrix is equal to a target Gram matix. If you do not understand the question spend more time, please. If you want a hint after all, here is a [Telegram bot for you](https://telegram.me/rdl_hw7_bot) (send /hint to him)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second part\n",
    "We give you **two options** for the second part.\n",
    "\n",
    "**First one** (if you are lazy or do not have GPU do just this):\n",
    "- Implement **Mean** and **Covariance** matching functions instead of $Gram$ matching. That is: \n",
    "    - *Mean* is a vector of size `C` which containes means over feature maps\n",
    "    - *Covariance* matrix is a *Gram* matrix of $Feats-mean$\n",
    "- What is $Q$ now? \n",
    "- Generate texture and stylize with $mean$ loss only; with $mean$ + $Covariance$ loss. Plot the results, side by side (3 textures and 3 stylized). What do you think? Actually, $Gram$ matrix or $Mean$ or $Mean$ + $Covariance$ matrix can be thought as texture descriptors. Does $mean$ encoding have enough parameters to represent texures?  \n",
    "- ***OR*** come up with your method to remove spatial information instead of above.\n",
    "- Bonus: you can mix several styles, averaging their representations. It can be fun. Some examples are [here](https://github.com/jcjohnson/neural-style).\n",
    "\n",
    "**Second one** (hardcore):\n",
    "- Substitute gram matrices with discriminator as in GAN. That is, you match distributions matching gram matrices and discriminator is designed to match distributions. Probably $Q$, we have defined is weak or too constraintive. Neural network based discriminator should be more flexible in this sense.\n",
    "    - The procedure will be a little bit unusual: we will optimize NN inside optimization loop w.r.t. image.\n",
    "    - You need to define a pixel level discriminator (at each layer you have $WH$ objects, each with $C$ features). Basically it should decide whether a pixel came from style image or from current image $X$. \n",
    "    - So the process is like that: \n",
    "        - At each image optimization iteration update D (actually you do not need to do minibatches updates here, you can simulate fully-connected layers with 1x1 convolutions, softmax with sigmoids). You will need to find a trade-off, for how long and how frequent should updates be. \n",
    "        - Then propagate gradient just like in GAN when optimizing $G$ i.e. swap labels (another strategy is in [here](https://www.robots.ox.ac.uk/~vgg/rg/papers/Tzeng_ICCV2015.pdf), eq. 4).\n",
    "        - Let L-BFGS (or whatever, probably adam will be more stable) update $X$.\n",
    "    - Discriminator architecture is up to you. It's better to start with logistic regression which should emulate $Mean$ + $Cov$ matching (isn't it?). \n",
    "    - I tried this myself without content loss only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Do everything in this notebook, I need your code as well as the generated images**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HINTS: \n",
    "\n",
    "- In case you do not have GPU, you need to substitute the line:\n",
    "  \n",
    "  `from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer`\n",
    "\n",
    "   with\n",
    "\n",
    "   `from lasagne.layers import Conv2DLayer as ConvLayer`\n",
    "   \n",
    "   \n",
    "- If you do not have GPU, resize your images to 256x256 and no more. Even at this resolution it may take an hour. You can decrease the number of iterations if it takes too long. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR (theano.sandbox.cuda): ERROR: Not using GPU. Initialisation of device 1 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "ERROR:theano.sandbox.cuda:ERROR: Not using GPU. Initialisation of device 1 failed:\n",
      "initCnmem: cnmemInit call failed! Reason=CNMEM_STATUS_OUT_OF_MEMORY. numdev=1\n",
      "\n",
      "Using gpu device 0: Quadro K4200 (CNMeM is enabled with initial size: 30.0% of memory, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import pickle\n",
    "import skimage.transform\n",
    "import scipy\n",
    "\n",
    "from PIL import Image\n",
    "import cPickle\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from lasagne.utils import floatX\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "from lasagne.layers import InputLayer, DenseLayer, NonlinearityLayer\n",
    "from lasagne.layers.dnn import Conv2DDNNLayer as ConvLayer\n",
    "from lasagne.layers import Pool2DLayer as PoolLayer\n",
    "from lasagne.nonlinearities import softmax\n",
    "\n",
    "IMAGE_W = 600\n",
    "\n",
    "# Note: tweaked to use average pooling instead of maxpooling\n",
    "def build_model():\n",
    "    net = {}\n",
    "    net['input'] = InputLayer((1, 3, IMAGE_W, IMAGE_W))\n",
    "    net['conv1_1'] = ConvLayer(net['input'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['conv1_2'] = ConvLayer(net['conv1_1'], 64, 3, pad=1, flip_filters=False)\n",
    "    net['pool1'] = PoolLayer(net['conv1_2'], 2, mode='average_exc_pad')\n",
    "    net['conv2_1'] = ConvLayer(net['pool1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['conv2_2'] = ConvLayer(net['conv2_1'], 128, 3, pad=1, flip_filters=False)\n",
    "    net['pool2'] = PoolLayer(net['conv2_2'], 2, mode='average_exc_pad')\n",
    "    net['conv3_1'] = ConvLayer(net['pool2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_2'] = ConvLayer(net['conv3_1'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_3'] = ConvLayer(net['conv3_2'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['conv3_4'] = ConvLayer(net['conv3_3'], 256, 3, pad=1, flip_filters=False)\n",
    "    net['pool3'] = PoolLayer(net['conv3_4'], 2, mode='average_exc_pad')\n",
    "    net['conv4_1'] = ConvLayer(net['pool3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_2'] = ConvLayer(net['conv4_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_3'] = ConvLayer(net['conv4_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv4_4'] = ConvLayer(net['conv4_3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool4'] = PoolLayer(net['conv4_4'], 2, mode='average_exc_pad')\n",
    "    net['conv5_1'] = ConvLayer(net['pool4'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_2'] = ConvLayer(net['conv5_1'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_3'] = ConvLayer(net['conv5_2'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['conv5_4'] = ConvLayer(net['conv5_3'], 512, 3, pad=1, flip_filters=False)\n",
    "    net['pool5'] = PoolLayer(net['conv5_4'], 2, mode='average_exc_pad')\n",
    "\n",
    "    return net\n",
    "\n",
    "\n",
    "# helper functions for all my needs\n",
    "\n",
    "def prep_image(im):\n",
    "    if len(im.shape) == 2:\n",
    "        im = im[:, :, np.newaxis]\n",
    "        im = np.repeat(im, 3, axis=2)\n",
    "    h, w, _ = im.shape\n",
    "    if h < w:\n",
    "        im = skimage.transform.resize(im, (IMAGE_W, w*IMAGE_W/h), preserve_range=True)\n",
    "    else:\n",
    "        im = skimage.transform.resize(im, (h*IMAGE_W/w, IMAGE_W), preserve_range=True)\n",
    "\n",
    "    # Central crop\n",
    "    h, w, _ = im.shape\n",
    "    im = im[h//2-IMAGE_W//2:h//2+IMAGE_W//2, w//2-IMAGE_W//2:w//2+IMAGE_W//2]\n",
    "    \n",
    "    rawim = np.copy(im).astype('uint8')\n",
    "    \n",
    "    # Shuffle axes to c01\n",
    "    im = np.swapaxes(np.swapaxes(im, 1, 2), 0, 1)\n",
    "    \n",
    "    # Convert RGB to BGR\n",
    "    im = im[::-1, :, :]\n",
    "\n",
    "    im = im - MEAN_VALUES\n",
    "    return rawim, floatX(im[np.newaxis])\n",
    "\n",
    "def image2pixelarray(filepath):\n",
    "    im = Image.open(filepath).convert('RGB')\n",
    "    (width, height) = im.size\n",
    "    idata = list(im.getdata())\n",
    "    idata = np.array(idata)\n",
    "    idata = idata.reshape((height, width, idata.shape[1]))\n",
    "    return idata\n",
    "\n",
    "def progress(i, n, skip=100, mode=1):\n",
    "    if (i%skip == 0 or n < i + skip) and mode == 1:\n",
    "        if i + 1 < n:\n",
    "            out = \"\\r%s%%\" % \"{:5.2f}\".format(100*i/float(n))\n",
    "        else:\n",
    "            out = \"\\r100%\"\n",
    "        sys.stdout.write(out)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def dump_obj(obj, fname):\n",
    "    try:\n",
    "        f = file(fname, 'wb')\n",
    "        cPickle.dump(obj, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        return 0\n",
    "\n",
    "def load_obj(fname):\n",
    "    try:\n",
    "        f = file(fname, 'rb')\n",
    "        loaded_obj = cPickle.load(f)\n",
    "        f.close()\n",
    "        return loaded_obj\n",
    "    except Exception as e:\n",
    "        print e\n",
    "        return 0\n",
    "    \n",
    "def deprocess(x):\n",
    "    x = np.copy(x[0])\n",
    "    x += MEAN_VALUES\n",
    "\n",
    "    x = x[::-1]\n",
    "    x = np.swapaxes(np.swapaxes(x, 0, 1), 1, 2)\n",
    "    \n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def do_train(tr_fn, iterations, outfolder, show=True, autosave=100):\n",
    "    if not os.path.exists(outfolder):\n",
    "        os.mkdir(outfolder)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        loss = tr_fn()\n",
    "        progress(i, iterations, skip=1)\n",
    "        if not i%autosave:\n",
    "            cur = generated_image.get_value().astype('float64')\n",
    "            dump_obj(cur, os.path.join(outfolder,str(u+i)))\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "            s = 'iteration: ' + str(u+i) + ' loss: ' + str(loss)\n",
    "            h.append(s)\n",
    "            for i in h[-5:]:\n",
    "                print i\n",
    "            \n",
    "            if show:\n",
    "                plt.figure(figsize=(12,12))\n",
    "                plt.imshow(deprocess(cur), interpolation='nearest')\n",
    "                plt.show()\n",
    "    \n",
    "MEAN_VALUES = np.array([104, 117, 123]).reshape((3,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the normalized pretrained weights from:\n",
    "# https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg19_normalized.pkl\n",
    "# (original source: https://bethgelab.org/deepneuralart/)\n",
    "\n",
    "!wget https://s3.amazonaws.com/lasagne/recipes/pretrained/imagenet/vgg19_normalized.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build VGG net and load weights\n",
    "\n",
    "net = build_model()\n",
    "values = pickle.load(open('vgg19_normalized.pkl'))['param values']\n",
    "lasagne.layers.set_all_param_values(net['pool5'], values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'artwork/julie.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7efd15d07e1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mphoto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage2pixelarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'artwork/julie.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrawim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphoto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphoto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrawim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9dd5a90fafee>\u001b[0m in \u001b[0;36mimage2pixelarray\u001b[1;34m(filepath)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mimage2pixelarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0midata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode)\u001b[0m\n\u001b[0;32m   2270\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2272\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2274\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'artwork/julie.jpg'"
     ]
    }
   ],
   "source": [
    "photo = image2pixelarray('artwork/julie.jpg')\n",
    "rawim, photo = prep_image(photo)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(rawim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
