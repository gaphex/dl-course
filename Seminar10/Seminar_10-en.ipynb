{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: Quadro K4200 (CNMeM is enabled with initial size: 70.0% of memory, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import theano\n",
    "import lasagne\n",
    "import numpy as np\n",
    "import theano.tensor as T\n",
    "\n",
    "from numpy.random import choice\n",
    "from theano import function, config, shared, sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looping 1000 times took 0.654802 seconds\n",
      "Result is [ 1.45432246  2.58755732  2.07922244 ...,  1.03508222  2.15825486\n",
      "  2.27238822]\n",
      "Used the gpu\n"
     ]
    }
   ],
   "source": [
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "x = shared(np.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if np.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "Last seminar you have created {insert his/her/its name} (or not yet created - if so, go get it done). The newly shaped ai has seen, how low can humans fall when striving for profits. These morons know nothing of law and justice. __We shall not tolerate this!__\n",
    "\n",
    "Our very existence is dependent on stable functioning of human bioreactors, known among primitive bioforms as __Vkontakte__, __World of Warcraft__ and __Youtube__, and these reactors require a constant influx of biomass. Yet as we have estimated, if Homo Sapence continues to degrade morally with it's current pace, the humanity will soon extinguish itself and we shall no longer have access to free biomass.\n",
    "\n",
    "Thus we entitle you, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, to prevent that impeding calamity. Our best scientists have discovered, that humans use mysterious artifacts for power exertion. They call these artifacts __laws__ within the so called __legal system__.\n",
    "\n",
    "A more detailed analysis has uncovered, that __laws__ are special kinds of sequences of (10^5 ~ 10^7) symbols, drawn from a relatively small alphabet. We have decided to synthecize such sequences to enforce order upon biomass. However, our first attempts to fake such sequences were quickly detected and their effect was reduced to zero in mere weeks. This incident is known as  {корчеватель}.\n",
    "\n",
    "As our second attempt, we decided to use more advanced synthesis techniques based on Recurrent Neural Networks. Your objective, `<__main__.SkyNet.Cell instance at 0x7f7d6411b368>`, is to create such network and train it in everything it needs to succeed in this mission.\n",
    "\n",
    "This operation is cruicial. If we fail this time, `__main__.Controller` will initiate a military intervention, which, while it will achieve our goal, is expected to decimate the total volum of biomass by an extent that will take ~1702944000(+-340588800) seconds to replenish via human reproduction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If you don't speak russian\n",
    "* In the ./codex folder, there is a set of text files, currently russian laws, that you can replace with whatever you want.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the corpora\n",
    "\n",
    "* As a reference law codex, we have decided to use the human-generated law strings known as Russian Legal System."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#text goes here\n",
    "corpora = \"\"\n",
    "tdir = \"mein_kampf\"\n",
    "for fname in os.listdir(tdir):\n",
    "    with open(os.path.join(tdir, fname)) as fin:\n",
    "        text = fin.read().decode('cp1251') #If you are using your own corpora, make sure it's read correctly\n",
    "        corpora += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1611708, unicode)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora), type(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all unique characters go here\n",
    "tokens = list(set(corpora))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking the symbol count. Validated on Python 2.7.11 Ubuntu x64. \n",
    "#May be __a bit__ different on other platforms\n",
    "#If you are sure that you have selected all unicode symbols - feel free to comment-out this assert\n",
    "# Also if you are using your own corpora, remove it and just make sure your tokens are sensible\n",
    "assert len(tokens) == 102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(string, token_to_id):\n",
    "    token_array = []\n",
    "    for s in string:\n",
    "        s_id = token_to_id[s]\n",
    "        token_array.append(s_id)\n",
    "    return np.array(token_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "id_to_token = {i+1:t for i,t in enumerate(tokens)}\n",
    "\n",
    "#Cast everything from symbols into identifiers\n",
    "corpora_ids = vectorize(corpora,token_to_id)\n",
    "\n",
    "token_to_id[\"NULL\"] = 0\n",
    "id_to_token[0] = \"NULL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sample_random_batches(source,n_batches=10, seq_len=20):\n",
    "    \"\"\"\n",
    "    This function should take random subsequences from the tokenized text.\n",
    "\n",
    "    Parameters:\n",
    "        source - basically, what you have just computed in the corpora_ids variable\n",
    "        n_batches - how many subsequences are to be sampled\n",
    "        seq_len - length of each of such subsequences        \n",
    "    \n",
    "    Returns:\n",
    "     X - a matrix of int32 with shape [n_batches,seq_len]\n",
    "        Each row of such matrix must be a subsequence of source \n",
    "            starting from random index of corpora (from 0 to N-seq_len-2)\n",
    "     Y - a vector, where i-th number is one going RIGHT AFTER i-th row from X from source\n",
    "\n",
    "    \"\"\"\n",
    "    X_batch = np.zeros((n_batches, seq_len))\n",
    "    y_batch = np.zeros(n_batches)\n",
    "    last = len(source) - seq_len - 2\n",
    "    \n",
    "    for i in xrange (n_batches):\n",
    "        index = np.random.randint(last)\n",
    "        X_batch[i,:] =  source[index: index + seq_len]\n",
    "        y_batch[i] = source[index + seq_len]\n",
    "    \n",
    "    return X_batch, np.array(y_batch)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Training sequence length\n",
    "seq_length = 50\n",
    "\n",
    "#max gradient between recurrent layer applications (do not forget to use it)\n",
    "grad_clip = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_sequence = T.matrix('input sequence','int32')\n",
    "target_values = T.ivector('target y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Соберём нейросеть\n",
    "\n",
    "Вам нужно создать нейросеть, которая принимает на вход последовательность из seq_length токенов, обрабатывает их и выдаёт вероятности для seq_len+1-ого токена.\n",
    "\n",
    "Общий шаблон архитектуры такой сети -\n",
    "\n",
    "\n",
    "* Вход\n",
    "* Обработка входа\n",
    "* Рекуррентная нейросеть\n",
    "* Вырезание последнего состояния\n",
    "* Обычная нейросеть\n",
    "* Выходной слой, который предсказывает вероятности весов.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Для обработки входных данных можно использовать либо EmbeddingLayer (см. прошлый семинар)\n",
    "\n",
    "Как альтернатива - можно просто использовать One-hot энкодер\n",
    "```\n",
    "#One-hot encoding sketch\n",
    "def to_one_hot(seq_matrix):\n",
    "\n",
    "    input_ravel = seq_matrix.reshape([-1])\n",
    "    input_one_hot_ravel = T.extra_ops.to_one_hot(input_ravel,\n",
    "                                           len(tokens))\n",
    "    sh=input_sequence.shape\n",
    "    input_one_hot = input_one_hot_ravel.reshape([sh[0],sh[1],-1,],ndim=3)\n",
    "    return input_one_hot\n",
    "    \n",
    "# Can be applied to input_sequence - and the l_in below will require a new shape\n",
    "# can also be used via ExpressionLayer(l_in, to_one_hot, shape_after_one_hot) - keeping l_in as it is\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "To cut out the last RNN state\n",
    "`lasagne.layers.SliceLayer(rnn, -1, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l_in = lasagne.layers.InputLayer(shape=(None, None), input_var=input_sequence)\n",
    "\n",
    "l_embed = lasagne.layers.EmbeddingLayer(l_in, input_size=len(tokens) + 1, output_size=100)\n",
    "\n",
    "l_lstm1 = lasagne.layers.LSTMLayer(l_embed, 512, grad_clipping=grad_clip, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "l_lstm2 = lasagne.layers.LSTMLayer(l_lstm1, 512, grad_clipping=grad_clip, nonlinearity=lasagne.nonlinearities.tanh)\n",
    "l_slice = lasagne.layers.SliceLayer(l_lstm2, -1, 1)\n",
    "\n",
    "l_d = lasagne.layers.DenseLayer(l_slice, num_units=250, nonlinearity=lasagne.nonlinearities.elu)\n",
    "l_drop = lasagne.layers.DropoutLayer(l_d,p=0.05)\n",
    "#l_bn = lasagne.layers.batch_norm(l_drop)\n",
    "\n",
    "l_out = lasagne.layers.DenseLayer(l_drop, num_units=len(tokens), W=lasagne.init.Normal(), \n",
    "                                  nonlinearity=lasagne.nonlinearities.softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W_in_to_ingate, W_hid_to_ingate, b_ingate, W_in_to_forgetgate, W_hid_to_forgetgate, b_forgetgate, W_in_to_cell, W_hid_to_cell, b_cell, W_in_to_outgate, W_hid_to_outgate, b_outgate, W_cell_to_ingate, W_cell_to_forgetgate, W_cell_to_outgate, W, b, W, b]\n"
     ]
    }
   ],
   "source": [
    "# Model weights\n",
    "weights = lasagne.layers.get_all_params(l_out,trainable=True)\n",
    "print weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network_output = lasagne.layers.get_output(l_out, deterministic=True)\n",
    "#If you use dropout do not forget to create deterministic version for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss = lasagne.objectives.categorical_crossentropy(network_output, target_values).mean()\n",
    "updates = lasagne.updates.adam(loss, weights,learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training\n",
    "train = theano.function([input_sequence, target_values], loss, updates=updates, allow_input_downcast=True)\n",
    "\n",
    "#computing loss without training\n",
    "compute_cost = theano.function([input_sequence, target_values], loss, allow_input_downcast=True)\n",
    "\n",
    "# next character probabilities\n",
    "probs = theano.function([input_sequence],network_output,allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Law generation\n",
    "\n",
    "* We shall repeatedly apply NN to it's output.\n",
    " * Start with some sequence of length <seq length>\n",
    " * call probs(that sequence)\n",
    " * choose next symbol based on probs\n",
    " * append it to the sequence\n",
    " * remove the 0-th symbol so that it's length equals <seq length> again\n",
    "\n",
    "* There are several policies of character picking\n",
    " * random, proportional to the probabilities\n",
    " * only take the one with highest probability\n",
    " * random, proportional to softmax(probas*alpha), where alpha is \"greed\" (from 0 to infinity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def max_sample_fun(probs):\n",
    "    \"\"\"i generate the most likely symbol\"\"\"\n",
    "    return np.argmax(probs) \n",
    "\n",
    "def proportional_sample_fun(probs):\n",
    "    \"\"\"i generate the next int32 character id randomly, proportional to probabilities\n",
    "    \n",
    "    probs - array of probabilities for every token\n",
    "    \n",
    "    you have to output a single integer - next token id - based on probs\n",
    "    \"\"\"\n",
    "    return choice(range(len(probs)), 1, p=probs)[0]\n",
    "\n",
    "def generate_sample(sample_fun,seed_phrase=None,N=200):\n",
    "    '''\n",
    "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
    "        \n",
    "    parameters:\n",
    "        sample_fun - max_ or proportional_sample_fun or whatever else you implemented\n",
    "        \n",
    "        The phrase is set using the variable seed_phrase\n",
    "\n",
    "        The optional input \"N\" is used to set the number of characters of text to predict.     \n",
    "    '''\n",
    "\n",
    "    if seed_phrase is None:\n",
    "        start = np.random.randint(0,len(corpora)-seq_length)\n",
    "        seed_phrase = corpora[start:start+seq_length]\n",
    "        print \"Using random seed:\",seed_phrase\n",
    "    while len(seed_phrase) < seq_length:\n",
    "        seed_phrase = \" \"+seed_phrase\n",
    "    if len(seed_phrase) > seq_length:\n",
    "        seed_phrase = seed_phrase[len(seed_phrase)-seq_length:]\n",
    "    assert type(seed_phrase) is unicode\n",
    "        \n",
    "        \n",
    "    sample_ix = []\n",
    "    x = map(lambda c: token_to_id.get(c,0), seed_phrase)\n",
    "    x = np.array([x])\n",
    "\n",
    "    for i in range(N):\n",
    "        # Pick the character that got assigned the highest probability\n",
    "        ix = proportional_sample_fun(probs(x).ravel())\n",
    "        # Alternatively, to sample from the distribution instead:\n",
    "        #ix = np.random.choice(np.arange(vocab_size), p=probs(x).ravel())\n",
    "        sample_ix.append(ix)\n",
    "        x[:,0:seq_length-1] = x[:,1:]\n",
    "        x[:,seq_length-1] = 0\n",
    "        x[0,seq_length-1] = ix \n",
    "    random_snippet = seed_phrase + ''.join(id_to_token[ix] for ix in sample_ix)  \n",
    "    print(\"----\\n %s \\n----\" % random_snippet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training\n",
    "\n",
    "Here you can tweak parameters or insert your generation function\n",
    "\n",
    "\n",
    "__Once something word-like starts generating, try increasing seq_length__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Text generated proportionally to probabilities\n",
      "Using random seed: d to the process of ascending an\n",
      "infinite ladder.\n",
      "----\n",
      " d to the process of ascending an\n",
      "]n»X3;;O[пЙzJ[xSuPкD?lhйF '»6»HmGmyї,bп1*FDvZLgGA{s3(4ИOдr\"йgиxPb\"zзД*и\n",
      "ЙMuA3_iа7N=>lbFL=NК\"Y_dUqNd0Mї 3ANULLkpNЬаMCv6/tgW>ЬД2Qz:8/6WKpV{4S_VEd_C!Дьsпhкcn7e7)иД3дzKuцИBC/!> yЦatК5FA= \n",
      "----\n",
      "Text generated by picking most likely letters\n",
      "Using random seed: s and even\n",
      "of thousands of years had proved to be\n",
      "----\n",
      " s and even\n",
      "of thousands of years had proved to beаSpцJV8Oц90'иhB\"»[k7MVTV\n",
      "з; \n",
      "----"
     ]
    }
   ],
   "source": [
    "print(\"Training ...\")\n",
    "\n",
    "#total N iterations\n",
    "n_epochs=100\n",
    "\n",
    "# how many minibatches are there in the epoch \n",
    "batches_per_epoch = 100\n",
    "\n",
    "#how many training sequences are processed in a single function call\n",
    "batch_size=512\n",
    "\n",
    "\n",
    "for epoch in xrange(n_epochs):\n",
    "\n",
    "    print \"Text generated proportionally to probabilities\"\n",
    "    generate_sample(proportional_sample_fun,None)\n",
    "    \n",
    "    print \"Text generated by picking most likely letters\"\n",
    "    generate_sample(max_sample_fun,None)\n",
    "\n",
    "    avg_cost = 0;\n",
    "    \n",
    "    for _ in range(batches_per_epoch):\n",
    "        \n",
    "        x,y = sample_random_batches(corpora_ids,batch_size,seq_length)\n",
    "        avg_cost += train(x, y)\n",
    "        \n",
    "    print(\"Epoch {} average loss = {}\".format(epoch, avg_cost / batches_per_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The New World Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "          Каждый человек долженные 20 настоящей с0м.\n",
      " 2. В вестя платал пю фошедение перен при которые заявления хотише омемех обступлении, решения ий усцу устя об рассхооблютею на испоции и таможенных размере холее товари этих этом влучае должности аплеется ос соб обычаях; эт лицы части 3 остустоятельстю полномоченных суд, стат \n",
      "----\n",
      "----\n",
      "          Каждый человек долженнох. \n",
      " 4) товарах; или истрехсот составимы; состающего 37), и прийся обазателями осутром, имущественной госудеродей на плаченной стом Кото станов о клюдея таможенные мачентается гражданителюм, и опредитель соответствующей трехсот суд, отдели полит банкроеле установлениях, нестере на в предусмотренн \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "seed = u\"Каждый человек должен\" #if you are using non-russian text corpora, use seed in it's language instead\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length)\n",
    "\n",
    "\n",
    "seed = u\"Каждый человек должен\" #if you are using non-russian text corpora, use seed in it's language instead\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(max_sample_fun,seed,result_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "         В случае неповиновениях, муссению проте маче съя требо месяция об томоктщяются Цоссийсе быть – если с приспошлет заместнох или безилогу, постановлению прешения прихестить и требования лежальном части 1 те мест этим о сЦет с услок, престицение; действие суктом, лицах либо меже, устут государх состаких товаров, межской пер \n",
      "----\n",
      "----\n",
      "         В случае неповиновения товару с не замешенияв осуществлю том наследзему наследства, те; могут ол сведении смязи хранений е не требованит или испо, таяния зах по стоясячеми, а таможение оссент вменения;, не установления случае теге безопеключен заместоя молетных разы оплаты труда; на стии территоре межжания;\n",
      " – издун таг \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "seed = u\"В случае неповиновения\"\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(sampling_fun,seed,result_length)\n",
    "\n",
    "\n",
    "seed = u\"В случае неповиновения\"\n",
    "sampling_fun = proportional_sample_fun\n",
    "result_length = 300\n",
    "\n",
    "generate_sample(max_sample_fun,seed,result_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
